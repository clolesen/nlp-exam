---
title: 'NLP Exam Project: Author collaborations in cognitive science'
author: "Christoffer Lundbak Olesen"
date: "01/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
LIBRARIES
```{r}
library(tidyverse)
library(textmineR)
library(LaplacesDemon)
library(data.table)
library(tictoc)
```

GET RESULTS
```{r}
#Load meta data
path = "data/processed/cognition"
obj_path = "data/R_objects/cognition"
metadata = fread(paste0(path,"/metadata.csv"))

#Run the analysis with k = 200
#RUN THE FUNCTION BLOC BELOW FIRST
Results = RunItAll(200, metadata)
```

FUNCTIONS
```{r}
# Function that prepares data for analysis
prepareData = function(metadata){
  #Loop through docs and create a corpus in the desired format
  doc_list = list()
  i = 1
  for (id in metadata$ID){
    doc = read.csv(paste0(path,"/1gram/",id,".csv"), stringsAsFactors = F)
    wVec = c()
    for (r in 1:length(doc$word)){
      wVec = c(wVec, rep(doc[r,2],doc[r,3]))
    }
    wStr = paste(wVec,collapse = " ")
    row = data.frame(docID = i, string = wStr)
    doc_list[[i]] = row
    i = i + 1
  }
  corpus = do.call(rbind, doc_list)
  save(corpus, file = paste0(obj_path,"/corpus.Rdata"))
  
  # make a list of authors containing the ID's of articles they wrote/co-wrote
  author_list = list()
  for(r in 1:length(metadata[,1][[1]])){ 
    
    docID = r
    names_str = metadata[r,"names"]
    names = str_split(names_str, ",", simplify = T)
    names = names[names!=""]
    
    for (name in names){
      if (name %in% names(author_list)){
        author_list[[name]] = c(author_list[[name]], docID)
      } else {
        author_list[length(author_list)+1] = docID
        names(author_list) = c(names(author_list[1:length(author_list)-1]),name)
      }
    }
  }
  save(author_list, file = paste0(obj_path,"/author_list.Rdata"))
  
  data_list = list(corpus,author_list)
  return(data_list)
}

# Function that runs an LDA model on the corpus and makes ATMs for each athour (ATM over time)
TopicModels = function(k,corpus,metadata, author_list){

  # create a document term matrix 
  dtm = CreateDtm(doc_vec = corpus$string, # character vector of documents
                  doc_names = corpus$docID, # document names
                  stopword_vec = c(),
                  lower = F, # lowercase
                  remove_punctuation = F,
                  remove_numbers = T, 
                  verbose = F) # Turn off status bar
  
  #Remove words that only accur once or twice in each doc
  dtm = dtm[,colSums(dtm) > 2]
  
  # Make topics
  LDAmodel = FitLdaModel(dtm = dtm, 
                         k = k,
                         iterations = 350,
                         burnin = 200,
                         alpha = 0.1,
                         beta = 0.05,
                         optimize_alpha = TRUE,
                         calc_likelihood = TRUE,
                         calc_coherence = TRUE,
                         calc_r2 = TRUE,
                         cpus = 2) 
  
  save(LDAmodel, file = paste0(obj_path,"/LDAmodel.Rdata"))
  print("LDA done")
  
  # Make author topic models
  ATM_list = list()
  i = 1
  for (A in author_list){
    docs = matrix(c(A,metadata[A,"time"][[1]]),ncol = 2) # make a matrix that contains docs and times
    if (length(docs[,1])!=1) docs = docs[order(docs[,2]),] # Order the docs in time
    
    ATM = matrix(nrow = length(A), ncol = k+2) # create matrix for putting in data
    PubNum = 1 # Publication number (start with the first)
    for (doc in docs[,1]){ #loop through docs
      topics = LDAmodel$theta[doc,] #Topic destribution of the doc 
      if (PubNum == 1) row = topics else { # if its the first doc, then the ATM row is the same as the docs topic distribution. Else:
        lastRow = ATM[PubNum-1,1:k] # Take the mean of all previous ATM rows
        sum = lastRow*(PubNum-1)+topics # (here we reverse engineer the mean calculation)
        row = sum/PubNum
      }
      row = c(row, doc, docs[PubNum,2]) # Add the doc id and the time point to the row
      ATM[PubNum,] = row # put the row in the ATM
      PubNum = PubNum + 1 # next publication
    }
    ATM_list[[i]] = ATM # put the ATM in the ATM_list
    names(ATM_list)[i] = names(author_list[i]) # give the entry the author name
    i = i + 1
  }
  save(ATM_list, file = paste0(obj_path,"/ATM_list.Rdata"))
  print("ATM done")
  
  results = list(LDAmodel,ATM_list)
  return(results)
}

# Function that calculates suprise and diversity measure and returns the main results and the author history 
getSurDiv = function(ATM_list, LDAmodel, metadata, k, skipHist = F){
  
  
  idCol = k + 1 # column index of the doc ids in the ATMs
  timeCol = k + 2  # column index of the time variable in the ATMs
  
  mainResults_list = list()
  
  if (skipHist == F){
  
    # MAKE AUTHOR HISTORY
  authorHistory_list = list()
  i = 1
  for (name in names(ATM_list)){ # Loop through names of authors
    ATM = ATM_list[[name]] # the ATM of the given author
    
    # Prepare lists for surprise measures
    SurToPrev_list = c() 
    SurFromPrev_list = c()
    SurToFirst_list = c()
    SurFromFirst_list = c()
    SurToLast_list = c()
    SurFromLast_list = c()
    SurToNext_list = c()
    SurFromNext_list = c()
    
    # loop through the authors publications
    for (PubNum in 1:length(ATM[,1])){
      if (PubNum == 1){ #the first publication does not have surprise in relation to the first or the previous publication, so it is set to 0
        SurToPrev = 0
        SurFromPrev = 0
        SurToFirst = 0
        SurFromFirst = 0
      } else { # ELSE calculate these surprise values
        KLprev = KLD(ATM[PubNum-1,1:k],ATM[PubNum,1:k])
        KLfirst = KLD(ATM[1,1:k],ATM[PubNum,1:k])
        
        SurFromPrev = KLprev$sum.KLD.px.py
        SurToPrev = KLprev$sum.KLD.py.px
        
        SurFromFirst = KLfirst$sum.KLD.px.py
        SurToFirst = KLfirst$sum.KLD.py.px
      }
      if (PubNum == length(ATM[,1])){ # The last publication 
        SurToLast = 0
        SurFromLast = 0
        SurToNext = 0
        SurFromNext = 0
      } else { # Calculate suprise values
        KLlast = KLD(ATM[length(ATM[,1]),1:k],ATM[PubNum,1:k])
        KLnext = KLD(ATM[PubNum+1,1:k],ATM[PubNum,1:k])
        
        SurFromNext = KLnext$sum.KLD.px.py
        SurToNext = KLnext$sum.KLD.py.px
        
        SurFromLast = KLlast$sum.KLD.px.py
        SurToLast = KLlast$sum.KLD.py.px
      }
      
      # Store surprise values
      SurToPrev_list = c(SurToPrev_list,SurToPrev)
      SurFromPrev_list = c(SurFromPrev_list,SurFromPrev)
      SurToFirst_list = c(SurToFirst_list,SurToFirst)
      SurFromFirst_list = c(SurFromFirst_list,SurFromFirst)
      SurToLast_list = c(SurToLast_list,SurToLast)
      SurFromLast_list = c(SurFromLast_list,SurFromLast)
      SurToNext_list = c(SurToNext_list,SurToNext)
      SurFromNext_list = c(SurFromNext_list,SurFromNext)
    }
    
    #put the author history in the list
    authorHistory_list[[i]] = data.table(
      Author = name,
      DocID = ATM[,idCol],
      nColllab = metadata$ncon[ATM[,idCol]],
      PubNum = 1:length(ATM[,1]),
      MaxPubNum = length(ATM[,1]),
      time = ATM[,timeCol],
      SurToPrev= SurToPrev_list,
      SurFromPrev = SurFromPrev_list,
      SurToFirst = SurToFirst_list,
      SurFromFirst = SurFromFirst_list,
      SurToLast = SurToLast_list,
      SurFromLast = SurFromLast_list,
      SurToNext = SurToNext_list,
      SurFromNext = SurFromNext_list
    )
    
    i = i + 1
  }
  save(authorHistory_list, file = paste0(obj_path,"/authorHistory_list.Rdata"))
  authorHistory = do.call(rbind, authorHistory_list)
  print("Author history - done")

  } else {authorHistory = "NO AUTHOR HISTORY HERE"}
  
  #Identify all collaborations
  collabs = 1:length(metadata[,1][[1]])
  collabs = collabs[metadata$ncon > 1] 
  
  # CALCULATE MAIN RESULTS
  theta = LDAmodel$theta # Extract theta from the LDA model
  ii = 1
  for (doc in collabs){ # loop through collaborations
    
    docTopics = theta[doc,] #identify the topics of the document
    
    # Get names of authors
    names = str_split(metadata[doc,"names"], ",", simplify = T)
    names = names[names!=""]
    
    i = 1
    
    # prepare lists
    KLs = c()
    subATM_list = list()
    nPub = c()
    time_list = c()
    timeDifSur_list = c()
    
    # loop through authors of the doc
    for (name in names){
      ATM = ATM_list[[name]] # Get ATM
      PubNum = match(doc,ATM[,idCol]) # Identify this docs number whithin the authors publications
      if (PubNum > 1){ # If it is not the authors first publication
        subATM = ATM[PubNum-1,1:k] # Get the ATM at the previous time point
        KLdiv = KLD(subATM,docTopics)$sum.KLD.px.py # Calculate the KL divergence
        time_list = c(time_list, ATM[PubNum-1,timeCol]) #store the previous time point (time vairable)
        subATM_list[[i]] = subATM # store the ATM at the previous time point
        
        # get time difference for the docs used in the surprise meesure
        timeSur = c(metadata[[doc,"time"]],ATM[PubNum-1,timeCol]) 
        timeDifSur = max(timeSur)-min(timeSur)
        timeDifSur_list = c(timeDifSur_list, timeDifSur)
      } else { # If it IS the authors first publication
        KLdiv = 0
        time_list = c(time_list, 0)
        subATM_list[[i]] = rep(0,k)
        timeDifSur_list = c(timeDifSur_list, 0)
      }
      # Store vairables
      KLs = c(KLs,KLdiv)
      nPub = c(nPub, PubNum-1)
      i = i + 1
    }
    # Get Surprise (minimum KL-divergence)
    surprise = min(KLs)
    
    # Prepare for diversity
    div_list = c()
    timeDif_list = c()
    i = 1
    # loop through authors of the document
    for (n1 in 1:length(names)){
      if (i == length(names)) break #Break the loop when getting to the last author (in order not to calculate anything twice)
      i = i + 1
      for (n2 in i:length(names)){ # Loop thorugh all subsequent authors
        div = KLD(subATM_list[[n1]],subATM_list[[n2]])$intrinsic.discrepancy # calculate intrinsict descrpency of KL-divergence
        
        div_list = c(div_list, div) # store values
        
        # get time differences for the authors in the diversity measure
        times = c(time_list[n1], time_list[n2])
        timeDif = max(times)-min(times)
        timeDif_list = c(timeDif_list,timeDif)
      }
      
    }
    # Get diversity
    diversity = max(div_list)
    
    # Store the results in a list of rows
    mainResults_list[[ii]] = data.table(
      docID = doc,
      nCollab = metadata$ncon[doc],
      Diversity = diversity,
      Surprise = surprise,
      docTime = metadata[[doc,"time"]],
      maxTimePrev = max(time_list),
      minTimePrev = min(time_list),
      meanTimeDifDiveristy = mean(timeDif_list),
      meanTimeDifSurprise = mean(timeDifSur_list),
      minPrevPub = min(nPub),
      maxPrevPub = max(nPub)
    )
    ii = ii + 1  
  } # END loop through collabs
  
  mainResults = do.call(rbind, mainResults_list)
  if (skipHist == F) save(mainResults, file = paste0(obj_path,"/mainResults.Rdata"))
  print("Main results - done")
  
  
  results = list(mainResults,authorHistory)
  return(results)
}

# Function that runs all above function in correct order and measures how long it takes in time
RunItAll = function(k,metadata){
  tic("total run time")
  
  tic("prepare data")
  data = prepareData(metadata)
  toc()
  
  corpus = data[[1]]
  author_list = data[[2]]
  
  
  tic("Topic modeling")
  TM = TopicModels(k, corpus, metadata, author_list)
  toc()
  
  LDAmodel = TM[[1]]
  ATM_list = TM[[2]]
  
  tic("Caluculate Diversity and Surprise")
  surDiv = getSurDiv(ATM_list, LDAmodel, metadata, k)
  toc()
  
  mainResults = surDiv[[1]]
  authorHistory = surDiv[[2]]
  
  results = list(mainResults, authorHistory, LDAmodel, ATM_list)
  
  toc()
  
  return(results)
}
```

GENERATE NEW DOCS
```{r}
topic_list = names(Results[[3]]$theta[1,])
word_list = names(Results[[3]]$phi[1,])
t_list = c()  
newDocs_list = list()
i = 1
n = 1
k = 150
for (id in subset(Results[[1]], minPrevPub > 1 & nCollab %in% 2:4)$docID){
  tic()
  names_str = metadata[id,"names"]
  names = str_split(names_str, ",", simplify = T)
  names = names[names!=""]
  
  nCollab = metadata[id,"ncon"][[1]]
  len = length(str_split(corpus$string[id], " ", simplify = T)) 
  nWords = round(len/nCollab)
  ii = 1
  for (x in 1:1){
    newDoc = c()
    for (name in names){
      ATM = Results[[4]][[name]]
      prev = which(ATM[,k+1] == id) - 1
      prevTopics = ATM[prev,1:k]
      
      for (wNum in 1:nWords){
        topic = sample(topic_list, size = 1, prob = prevTopics)
        word = sample(word_list, size = 1, prob = Results[[3]]$phi[topic,])
        
        newDoc = c(newDoc, word)
      }
    }
    newDocID = id + (ii * 10^-3)
      row = data.table(docID = newDocID, string = paste(newDoc, collapse = " "))
      newDocs_list[[i]] = row
      i = i + 1
      ii = ii + 1
  }
  print(paste(n,"out of 1611 -",1611-n,"left", sep = " "))
  n = n + 1
  t = toc()
  t = t$toc[[1]] - t$tic[[1]]
  t_list = c(t_list,t)
  est = (mean(t_list)*(1611-n))/60/60
  print(paste("Estimated time left:",est,"hours", sep = " "))
}
newDocs = do.call(rbind, newDocs_list)


# create a document term matrix 
newDtm = CreateDtm(doc_vec = newDocs$string, # character vector of documents
                doc_names = newDocs$docID, # document names
                stopword_vec = c(),
                lower = F, # lowercase
                remove_punctuation = F,
                remove_numbers = F, 
                verbose = T) # Turn on status bar

# Predict LDA topics on new documents
newTheta = predict(Results[[3]], newDtm, iterations = 350, burnin = 200)
save(newTheta, file = paste0(obj_path,"/newTheta.Rdata"))

# Suprise ~ Diversity analysis on new documents
k = 150
idCol = k + 1
timeCol = k + 2
newResults_list = list()
ii = 1
for (doc in newDocs$docID){
  docTopics = newTheta[ii,]
  
  oDoc = as.integer(doc)
  
  names = str_split(metadata[oDoc,"names"], ",", simplify = T)
  names = names[names!=""]
  KLs = c()
  i = 1
  subATM_list = list()
  nPub = c()
  time_list = c()
  timeDifSur_list = c()
  for (name in names){
    ATM = Results[[4]][[name]]
    PubNum = match(oDoc,ATM[,idCol])
    if (PubNum > 1){
      subATM = ATM[PubNum-1,1:k] 
      KLdiv = KLD(subATM,docTopics)$sum.KLD.px.py
      subATM_list[[i]] = subATM
    } else {
      KLdiv = 0
      subATM_list[[i]] = rep(0,k)
    }
    KLs = c(KLs,KLdiv)
    nPub = c(nPub, PubNum-1)
    i = i + 1
  }
  surprise = min(KLs)
  
  div_list = c()
  i = 1
  for (n1 in 1:length(names)){
    if (i == length(names)) break
    i = i + 1
    for (n2 in i:length(names)){
      div = KLD(subATM_list[[n1]],subATM_list[[n2]])$intrinsic.discrepancy
      div_list = c(div_list, div)
    }
    
  }
  diversity = max(div_list)
  
  
  newResults_list[[ii]] = data.table(
    docID = doc,
    nCollab = metadata$ncon[doc],
    Diversity = diversity,
    Surprise = surprise,
    docTime = metadata[[doc,"time"]],
    minPrevPub = min(nPub),
    maxPrevPub = max(nPub)
  )
  ii = ii + 1  
} # END loop through collabs
newResults = do.call(rbind, newResults_list)
save(newResults, file = paste0(obj_path,"/newReuslts.Rdata"))
```

